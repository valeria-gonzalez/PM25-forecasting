{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting PM2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Reframing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective \n",
    "\n",
    "Once the data has been properly cleaned, a specific pollutant can be selected to forecast using a ML model. \n",
    "\n",
    "PM2.5 will be selected given that it's the pollutant with the most complete information and also it has great relevance as a main pollutant in Tlaquepaque.\n",
    "\n",
    "Reframing the dataset involves reformating the information for supervised learning, i.e., a set of variables will be used as dependent variables (to give context) and they will each have an independent variable (result) assigned to them. For time series forecasting, it's important to also group these dependent and independent variables in windows of time. \n",
    "\n",
    "This allows the model to generalize the information of time (t-1) and use it to predict the results of time (t)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forecasting PM2.5\n",
    "\n",
    "To forecast PM2.5, the features used will be:\n",
    "\n",
    "_Exogenous variables (dependent variable):_\n",
    "\n",
    "- Temperature (TMP)\n",
    "- Relative Humidity (RH)\n",
    "- Barometric Pressure (PBA)\n",
    "- Wind Speed (WS)\n",
    "- Wind Direction (WD)\n",
    "\n",
    "_Pollutant (independent variable):_\n",
    "\n",
    "- Fine Particulate Matter less than 2.5 micrometers (PM2.5)\n",
    "\n",
    "This requires a multivariate model, which in this case will be LTSM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "import numpy as np # Numerical operations\n",
    "import pandas as pd # Dataset manipulation\n",
    "\n",
    "import seaborn as sns # Graphs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler # Scale data\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM # Model\n",
    "from tensorflow.keras.layers import Dense, Dropout # Layer\n",
    "from tensorflow.keras.optimizers import Adam # Optimizer function\n",
    "\n",
    "from sklearn.metrics import mean_squared_error # Performance metrics\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import optuna # Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"semadet-aire-final\"\n",
    "filepath = f\"datasets/feature_eng/{filename}.csv\"\n",
    "\n",
    "# Read dataset\n",
    "df = pd.read_csv(filepath, parse_dates=[0], index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select pollutant to predict and exogenous variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PM2.5 will be the forecasted pollutant using 5 features (__Temperature (TMP), Relative Humidity (RH), Barometric Pressure (PBA), Wind Speed (WS), Wind Direction (WD)__) to help give the model context about when certain readings occur. The information on these features need to be extracted from the rest of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependent variables\n",
    "features = [\"pm25\", \"tmp\", \"rh\", \"ws\", \"wd\"]\n",
    "\n",
    "# Independent variable\n",
    "pollutant = \"pm25\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with specified features\n",
    "def select_df_features(df:pd.DataFrame, features:list)->pd.DataFrame:\n",
    "    df_select = pd.DataFrame()\n",
    "    for feature in features:\n",
    "        df_select[feature] = df[feature]\n",
    "    return df_select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe\n",
    "df_select = select_df_features(df, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_index(ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pm25</th>\n",
       "      <th>tmp</th>\n",
       "      <th>rh</th>\n",
       "      <th>ws</th>\n",
       "      <th>wd</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-12-31</th>\n",
       "      <td>35.319500</td>\n",
       "      <td>20.53</td>\n",
       "      <td>38.018182</td>\n",
       "      <td>2.31</td>\n",
       "      <td>176.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-30</th>\n",
       "      <td>23.888571</td>\n",
       "      <td>20.53</td>\n",
       "      <td>38.018182</td>\n",
       "      <td>2.31</td>\n",
       "      <td>176.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-29</th>\n",
       "      <td>27.874167</td>\n",
       "      <td>20.53</td>\n",
       "      <td>38.018182</td>\n",
       "      <td>2.31</td>\n",
       "      <td>176.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 pm25    tmp         rh    ws     wd\n",
       "date                                                \n",
       "2023-12-31  35.319500  20.53  38.018182  2.31  176.8\n",
       "2023-12-30  23.888571  20.53  38.018182  2.31  176.8\n",
       "2023-12-29  27.874167  20.53  38.018182  2.31  176.8"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Observe first three instances\n",
    "df_select.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data is separated, the data will be transformed to a common scale to improve the model's performance (scaling). \n",
    "Normalizing the data allows each feature to have values between a certain range, and depending on the activation functions the model uses, it could improve performance.\n",
    "\n",
    "In this case, the data held for each feature will be scaled to have values between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data scaler\n",
    "data_scaler = MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "# Scale the information in each column\n",
    "data_norm = data_scaler.fit_transform(df_select.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.34980187, 0.49707058, 0.35821323, 0.39591107, 0.50222254],\n",
       "       [0.20935121, 0.49707058, 0.35821323, 0.39591107, 0.50222254],\n",
       "       [0.25832181, 0.49707058, 0.35821323, 0.39591107, 0.50222254],\n",
       "       [0.18643834, 0.49707058, 0.35821323, 0.39591107, 0.50222254],\n",
       "       [0.11183536, 0.49707058, 0.35821323, 0.39591107, 0.50222254]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Observe the first 5 instances\n",
    "data_norm[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timeseries to supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised learning involves training and evaluating a model with information that is already has the correct result specified.\n",
    "\n",
    "For the existing dataset, the dataset needs to be separated in instances of the five dependent features labeled with the correct result, which is the PM2.5 reading for a given day. This is already done.\n",
    "\n",
    "Since this is also a time series forecasting problem, the data needs to be further split into windows of time. This means creating two overlapping groups of n and m instances, where:\n",
    "\n",
    "- Group one has n past instances of the all variables to learn from\n",
    "- Group two has m future instances of the desired variable to forecast (PM2.5)\n",
    "\n",
    "These groups overlap because the first group has observations from [0,n],[1,n+1] and so on, the same happens for group 2.\n",
    "\n",
    "This allows the model to learn how PM2.5 will change given certain circumstances, as well as use previously learned predictions to make new ones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the data into two groups of windows of n instances \n",
    "def series_to_supervised(data:np.ndarray, column_names:list, \n",
    "                         n_dependent:int=1, n_pred:int=1, \n",
    "                         dropnan=True)->pd.DataFrame:\n",
    "    \n",
    "    # Obtain number of variables in data\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    \n",
    "    # Create a dataframe of the data\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Save shifted columns and their names\n",
    "    cols, names = list(), list()\n",
    "    \n",
    "    # Create the input sequence (t-n, ... t-1) number of past instances\n",
    "    for i in range(n_dependent, 0, -1):\n",
    "        # Shift all data in features i positions downwards\n",
    "        cols.append(df.shift(i))\n",
    "        \n",
    "        # Save the name of the column and how many positions it was shifted\n",
    "        names += [(f\"{column_names[j]}(t-{i})\") for j in range(n_vars)]\n",
    "    \n",
    "    # Forecast sequence (t, t+1, ... t+n) number of predictions\n",
    "    for i in range(0, n_pred):\n",
    "        # Shift all features i positions upwards\n",
    "        cols.append(df.shift(-i))\n",
    "        \n",
    "        # Save the name of the colum  and the times it was shifted\n",
    "        if i == 0:\n",
    "            names += [(f\"{column_names[j]}(t)\") for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [(f\"{column_names[j]}(t+{i})\") for j in range(n_vars)]\n",
    "    \n",
    "    # Join the resulting columns and give them column names\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    \n",
    "    # Drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    \n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtained reframed dataset\n",
    "data_reframed = series_to_supervised(data_norm, features, n_dependent=1, n_pred=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pm25(t-1)</th>\n",
       "      <th>tmp(t-1)</th>\n",
       "      <th>rh(t-1)</th>\n",
       "      <th>ws(t-1)</th>\n",
       "      <th>wd(t-1)</th>\n",
       "      <th>pm25(t)</th>\n",
       "      <th>tmp(t)</th>\n",
       "      <th>rh(t)</th>\n",
       "      <th>ws(t)</th>\n",
       "      <th>wd(t)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.349802</td>\n",
       "      <td>0.497071</td>\n",
       "      <td>0.358213</td>\n",
       "      <td>0.395911</td>\n",
       "      <td>0.502223</td>\n",
       "      <td>0.209351</td>\n",
       "      <td>0.497071</td>\n",
       "      <td>0.358213</td>\n",
       "      <td>0.395911</td>\n",
       "      <td>0.502223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.209351</td>\n",
       "      <td>0.497071</td>\n",
       "      <td>0.358213</td>\n",
       "      <td>0.395911</td>\n",
       "      <td>0.502223</td>\n",
       "      <td>0.258322</td>\n",
       "      <td>0.497071</td>\n",
       "      <td>0.358213</td>\n",
       "      <td>0.395911</td>\n",
       "      <td>0.502223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.258322</td>\n",
       "      <td>0.497071</td>\n",
       "      <td>0.358213</td>\n",
       "      <td>0.395911</td>\n",
       "      <td>0.502223</td>\n",
       "      <td>0.186438</td>\n",
       "      <td>0.497071</td>\n",
       "      <td>0.358213</td>\n",
       "      <td>0.395911</td>\n",
       "      <td>0.502223</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pm25(t-1)  tmp(t-1)   rh(t-1)   ws(t-1)   wd(t-1)   pm25(t)    tmp(t)  \\\n",
       "1   0.349802  0.497071  0.358213  0.395911  0.502223  0.209351  0.497071   \n",
       "2   0.209351  0.497071  0.358213  0.395911  0.502223  0.258322  0.497071   \n",
       "3   0.258322  0.497071  0.358213  0.395911  0.502223  0.186438  0.497071   \n",
       "\n",
       "      rh(t)     ws(t)     wd(t)  \n",
       "1  0.358213  0.395911  0.502223  \n",
       "2  0.358213  0.395911  0.502223  \n",
       "3  0.358213  0.395911  0.502223  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_reframed.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function creates the n prediction group using all variables, only the feature that is to be forecasted should be kept (PM2.5) in this group and the rest dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns we don't want to predict (i.e, drop all features that aren't pm25 for time t)\n",
    "data_reframed.drop(data_reframed.columns[[6,7,8,9]], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pm25(t-1)</th>\n",
       "      <th>tmp(t-1)</th>\n",
       "      <th>rh(t-1)</th>\n",
       "      <th>ws(t-1)</th>\n",
       "      <th>wd(t-1)</th>\n",
       "      <th>pm25(t)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.349802</td>\n",
       "      <td>0.497071</td>\n",
       "      <td>0.358213</td>\n",
       "      <td>0.395911</td>\n",
       "      <td>0.502223</td>\n",
       "      <td>0.209351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.209351</td>\n",
       "      <td>0.497071</td>\n",
       "      <td>0.358213</td>\n",
       "      <td>0.395911</td>\n",
       "      <td>0.502223</td>\n",
       "      <td>0.258322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.258322</td>\n",
       "      <td>0.497071</td>\n",
       "      <td>0.358213</td>\n",
       "      <td>0.395911</td>\n",
       "      <td>0.502223</td>\n",
       "      <td>0.186438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pm25(t-1)  tmp(t-1)   rh(t-1)   ws(t-1)   wd(t-1)   pm25(t)\n",
       "1   0.349802  0.497071  0.358213  0.395911  0.502223  0.209351\n",
       "2   0.209351  0.497071  0.358213  0.395911  0.502223  0.258322\n",
       "3   0.258322  0.497071  0.358213  0.395911  0.502223  0.186438"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See final reframed dataset\n",
    "data_reframed.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Divide dataset into training, validation and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The machine learning model needs to be trained on different sets of the data so that it doesn't become used to only one kind of data. The dataset used has 3 years worth of information and will be divided in the following way:\n",
    "\n",
    "- Training set: Year of 2023 and first nine months of 2020 (0.56%)\n",
    "- Validation set: Last three months of 2020 (0.10%)\n",
    "- Test set: Year of 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the dataset in train, validation and test set\n",
    "def divide_series(df: pd.DataFrame, train:float=0.65, val:float=0.15):\n",
    "    data_len = len(df)\n",
    "    train_size = int(data_len * train)\n",
    "    val_size = int(data_len * val)\n",
    "    \n",
    "    train_df = pd.DataFrame()\n",
    "    val_df = pd.DataFrame()\n",
    "    test_df = pd.DataFrame()\n",
    "    \n",
    "    for feature in df.columns:\n",
    "        train_df[feature] = df[feature][:train_size]\n",
    "        val_df[feature] = df[feature][train_size:train_size + val_size]\n",
    "        test_df[feature] = df[feature][train_size + val_size:]\n",
    "    \n",
    "\n",
    "    return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain \n",
    "train_df, val_df, test_df = divide_series(data_reframed, train=0.56, val=0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set len: 613\n",
      "Validation set len: 109\n",
      "Test set len: 373\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train set len: {len(train_df)}\")\n",
    "print(f\"Validation set len: {len(val_df)}\")\n",
    "print(f\"Test set len: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pm25(t-1)</th>\n",
       "      <th>tmp(t-1)</th>\n",
       "      <th>rh(t-1)</th>\n",
       "      <th>ws(t-1)</th>\n",
       "      <th>wd(t-1)</th>\n",
       "      <th>pm25(t)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.349802</td>\n",
       "      <td>0.497071</td>\n",
       "      <td>0.358213</td>\n",
       "      <td>0.395911</td>\n",
       "      <td>0.502223</td>\n",
       "      <td>0.209351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.209351</td>\n",
       "      <td>0.497071</td>\n",
       "      <td>0.358213</td>\n",
       "      <td>0.395911</td>\n",
       "      <td>0.502223</td>\n",
       "      <td>0.258322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.258322</td>\n",
       "      <td>0.497071</td>\n",
       "      <td>0.358213</td>\n",
       "      <td>0.395911</td>\n",
       "      <td>0.502223</td>\n",
       "      <td>0.186438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pm25(t-1)  tmp(t-1)   rh(t-1)   ws(t-1)   wd(t-1)   pm25(t)\n",
       "1   0.349802  0.497071  0.358213  0.395911  0.502223  0.209351\n",
       "2   0.209351  0.497071  0.358213  0.395911  0.502223  0.258322\n",
       "3   0.258322  0.497071  0.358213  0.395911  0.502223  0.186438"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into X and y sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data has been separated into these sets, they need to be further separated into X and y sets, which only means separating the dependent variables (not PM2.5) from the independent variables (PM2.5). Or in the time series case, windows of past n observations from m future observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the values for each set of data\n",
    "train_values = train_df.values\n",
    "val_values = val_df.values\n",
    "test_values = test_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the X set by using all columns except last for all instances\n",
    "# Create y set by only using the last column for all instances\n",
    "\n",
    "train_X, train_y = train_values[:,:-1], train_values[:,-1]\n",
    "val_X, val_y = val_values[:,:-1], val_values[:,-1]\n",
    "test_X, test_y = test_values[:,:-1], test_values[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape data for model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of M.L. models requiere the X set to be reshaped in the following format [n_instances, n_timesteps, n_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(613, 5)\n"
     ]
    }
   ],
   "source": [
    "# Shape before reshaping\n",
    "print(train_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape each set\n",
    "train_X = train_X.reshape(train_X.shape[0], 1, train_X.shape[1])\n",
    "val_X = val_X.reshape(val_X.shape[0], 1, val_X.shape[1])\n",
    "test_X = test_X.reshape(test_X.shape[0], 1, test_X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(613, 1, 5)\n"
     ]
    }
   ],
   "source": [
    "# Shape after reshaping\n",
    "print(train_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Define and fit the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data has been properly separated, the model can be created. Choosing a model that adapts to the data is an important step that often requieres various trials and some research. After reviewing many papers about similar tasks related to pollutant forecasting and air quality predictors, the Multiple Layer Perceptron and Long Short Term Memory Models seemed to yield the better performance for this particular type of task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why LTSM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Long Short Term Memory neural network is known for being able to learn and process long term sequences and patterns by choosing what data is important to retain during the training process. In more technical terms, it being a Recurrent Neural Network allows to propagate past information learned to the other layers during training and not just the information learned directly before. This makes the model especially adept at time series forecasting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching for optimal hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optuna is a hyperparameter tuning framework that allows the user to select various hyperparameter options for a model and evaluates the performance of each during a set amount of trials.\n",
    "\n",
    "#### Choosing hyperparameters\n",
    "\n",
    "During my research and different trials, I found that for this specific problem, the following hyperparameters were viable options:\n",
    "\n",
    "- Number of hidden layers\n",
    "    - Value: 1,2,3\n",
    "    - Opinion: I found that more layers can often be a disadvantage that can lead to poor learning, one is often best.\n",
    "\n",
    "- Learning rate:\n",
    "    - Value: 0.001, 0.1\n",
    "    - Opinion: A learning rate that is too high can cause information loss\n",
    "\n",
    "- Epochs:\n",
    "    - Value: 50, 100\n",
    "    - Opinion: Too many epochs can cause the network to overcorrect the error when propagating \n",
    "\n",
    "- Nodes per layer:\n",
    "    - Value: 5, 50, 12, 8\n",
    "    - Opinion: Less nodes is often better\n",
    "\n",
    "- Activation Function:\n",
    "    - Values: ReLu\n",
    "\n",
    "- Optimizer:\n",
    "    Value: Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPOptimizer:\n",
    "  \"\"\"\n",
    "  Class for hyperparameter optimizing for PM2.5 Forecaster.\n",
    "  \"\"\"\n",
    "  def __init__(self, input_dim:int, n_trials:int):\n",
    "    self.input_dim = input_dim\n",
    "    self.n_trials = n_trials\n",
    "    self.study = None\n",
    "    self.X_train = None\n",
    "    self.y_train = None\n",
    "    self.X_val = None\n",
    "    self.y_val = None\n",
    "\n",
    "  def create_model(self, trial):\n",
    "    model = Sequential()\n",
    "\n",
    "    # Hyperparameter selection\n",
    "    neurons = trial.suggest_categorical(\"neurons\", [5, 10, 8])\n",
    "    # dropout_rate = trial.suggest_float(\"dropout\", 0.1, 0.2, log=True)\n",
    "    \n",
    "    model.add(LSTM(neurons, \n",
    "                   input_shape=(self.X_train.shape[1], self.X_train.shape[2]),\n",
    "                   activation=\"relu\",\n",
    "                   dropout=0.2))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # Compile model\n",
    "    # learning_rate = trial.suggest_cat(\"learning_rate\", 0.001, 0., log=True)\n",
    "    learning_rate = 0.1\n",
    "    model.compile(optimizer=Adam(learning_rate), loss=\"mae\")\n",
    "\n",
    "    return model\n",
    "\n",
    "  def objective(self, trial):\n",
    "    model = self.create_model(trial)\n",
    "    epochs = trial.suggest_categorical(\"epochs\", [100])\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(\n",
    "        self.X_train,\n",
    "        self.y_train,\n",
    "        epochs=epochs,\n",
    "        validation_data=(self.X_val, self.y_val),\n",
    "        verbose=0,\n",
    "        batch_size=32,\n",
    "        )\n",
    "\n",
    "    # Evaluate on validation data\n",
    "    loss = model.evaluate(self.X_val, self.y_val, verbose=0)\n",
    "\n",
    "    return loss\n",
    "\n",
    "  def optimize(self, X_train:np.array, y_train:np.array, X_val, y_val:np.array):\n",
    "    self.X_train = X_train\n",
    "    self.y_train = y_train\n",
    "    self.X_val = X_val\n",
    "    self.y_val = y_val\n",
    "\n",
    "    self.study = optuna.create_study(direction=\"minimize\")\n",
    "    self.study.optimize(self.objective, n_trials = self.n_trials)\n",
    "\n",
    "    print(\"Best hyperparameters:\", self.study.best_params)\n",
    "    return self.study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-15 21:36:15,379] A new study created in memory with name: no-name-22a7ef69-0006-47fe-a23a-c894bf9f9490\n",
      "[I 2025-04-15 21:36:19,688] Trial 0 finished with value: 0.06411813199520111 and parameters: {'neurons': 5, 'epochs': 100}. Best is trial 0 with value: 0.06411813199520111.\n",
      "[I 2025-04-15 21:36:23,681] Trial 1 finished with value: 0.12294361740350723 and parameters: {'neurons': 8, 'epochs': 100}. Best is trial 0 with value: 0.06411813199520111.\n",
      "[I 2025-04-15 21:36:27,740] Trial 2 finished with value: 0.10214603692293167 and parameters: {'neurons': 10, 'epochs': 100}. Best is trial 0 with value: 0.06411813199520111.\n",
      "[I 2025-04-15 21:36:31,650] Trial 3 finished with value: 0.08625276386737823 and parameters: {'neurons': 5, 'epochs': 100}. Best is trial 0 with value: 0.06411813199520111.\n",
      "[I 2025-04-15 21:36:35,574] Trial 4 finished with value: 0.1324654221534729 and parameters: {'neurons': 8, 'epochs': 100}. Best is trial 0 with value: 0.06411813199520111.\n",
      "[I 2025-04-15 21:36:39,617] Trial 5 finished with value: 0.07123720645904541 and parameters: {'neurons': 10, 'epochs': 100}. Best is trial 0 with value: 0.06411813199520111.\n",
      "[I 2025-04-15 21:36:43,658] Trial 6 finished with value: 0.07166560739278793 and parameters: {'neurons': 5, 'epochs': 100}. Best is trial 0 with value: 0.06411813199520111.\n",
      "[I 2025-04-15 21:36:47,838] Trial 7 finished with value: 0.06680269539356232 and parameters: {'neurons': 8, 'epochs': 100}. Best is trial 0 with value: 0.06411813199520111.\n",
      "[I 2025-04-15 21:36:51,769] Trial 8 finished with value: 0.08270663768053055 and parameters: {'neurons': 5, 'epochs': 100}. Best is trial 0 with value: 0.06411813199520111.\n",
      "[I 2025-04-15 21:36:55,532] Trial 9 finished with value: 0.12032823264598846 and parameters: {'neurons': 5, 'epochs': 100}. Best is trial 0 with value: 0.06411813199520111.\n",
      "[I 2025-04-15 21:36:59,272] Trial 10 finished with value: 0.12512485682964325 and parameters: {'neurons': 5, 'epochs': 100}. Best is trial 0 with value: 0.06411813199520111.\n",
      "[I 2025-04-15 21:37:02,905] Trial 11 finished with value: 0.07326262444257736 and parameters: {'neurons': 8, 'epochs': 100}. Best is trial 0 with value: 0.06411813199520111.\n",
      "[I 2025-04-15 21:37:06,706] Trial 12 finished with value: 0.06294997036457062 and parameters: {'neurons': 8, 'epochs': 100}. Best is trial 12 with value: 0.06294997036457062.\n",
      "[I 2025-04-15 21:37:10,887] Trial 13 finished with value: 0.08382276445627213 and parameters: {'neurons': 8, 'epochs': 100}. Best is trial 12 with value: 0.06294997036457062.\n",
      "[I 2025-04-15 21:37:14,683] Trial 14 finished with value: 0.11144155263900757 and parameters: {'neurons': 10, 'epochs': 100}. Best is trial 12 with value: 0.06294997036457062.\n",
      "[I 2025-04-15 21:37:18,682] Trial 15 finished with value: 0.08744148910045624 and parameters: {'neurons': 5, 'epochs': 100}. Best is trial 12 with value: 0.06294997036457062.\n",
      "[I 2025-04-15 21:37:22,581] Trial 16 finished with value: 0.10075810551643372 and parameters: {'neurons': 8, 'epochs': 100}. Best is trial 12 with value: 0.06294997036457062.\n",
      "[I 2025-04-15 21:37:26,461] Trial 17 finished with value: 0.09607892483472824 and parameters: {'neurons': 8, 'epochs': 100}. Best is trial 12 with value: 0.06294997036457062.\n",
      "[I 2025-04-15 21:37:30,338] Trial 18 finished with value: 0.06491228938102722 and parameters: {'neurons': 5, 'epochs': 100}. Best is trial 12 with value: 0.06294997036457062.\n",
      "[I 2025-04-15 21:37:34,204] Trial 19 finished with value: 0.15119870007038116 and parameters: {'neurons': 10, 'epochs': 100}. Best is trial 12 with value: 0.06294997036457062.\n",
      "[I 2025-04-15 21:37:38,273] Trial 20 finished with value: 0.06139098107814789 and parameters: {'neurons': 8, 'epochs': 100}. Best is trial 20 with value: 0.06139098107814789.\n",
      "[I 2025-04-15 21:37:42,195] Trial 21 finished with value: 0.0767773762345314 and parameters: {'neurons': 8, 'epochs': 100}. Best is trial 20 with value: 0.06139098107814789.\n",
      "[I 2025-04-15 21:37:47,521] Trial 22 finished with value: 0.08357690274715424 and parameters: {'neurons': 8, 'epochs': 100}. Best is trial 20 with value: 0.06139098107814789.\n",
      "[I 2025-04-15 21:37:51,445] Trial 23 finished with value: 0.10335078090429306 and parameters: {'neurons': 8, 'epochs': 100}. Best is trial 20 with value: 0.06139098107814789.\n",
      "[I 2025-04-15 21:37:55,077] Trial 24 finished with value: 0.0801224634051323 and parameters: {'neurons': 8, 'epochs': 100}. Best is trial 20 with value: 0.06139098107814789.\n",
      "[I 2025-04-15 21:37:58,765] Trial 25 finished with value: 0.14240308105945587 and parameters: {'neurons': 5, 'epochs': 100}. Best is trial 20 with value: 0.06139098107814789.\n",
      "[I 2025-04-15 21:38:02,470] Trial 26 finished with value: 0.07557424157857895 and parameters: {'neurons': 8, 'epochs': 100}. Best is trial 20 with value: 0.06139098107814789.\n",
      "[I 2025-04-15 21:38:06,109] Trial 27 finished with value: 0.1316370815038681 and parameters: {'neurons': 8, 'epochs': 100}. Best is trial 20 with value: 0.06139098107814789.\n",
      "[I 2025-04-15 21:38:10,119] Trial 28 finished with value: 0.1076924055814743 and parameters: {'neurons': 10, 'epochs': 100}. Best is trial 20 with value: 0.06139098107814789.\n",
      "[I 2025-04-15 21:38:14,153] Trial 29 finished with value: 0.0789283886551857 and parameters: {'neurons': 5, 'epochs': 100}. Best is trial 20 with value: 0.06139098107814789.\n",
      "[I 2025-04-15 21:38:18,296] Trial 30 finished with value: 0.08302469551563263 and parameters: {'neurons': 8, 'epochs': 100}. Best is trial 20 with value: 0.06139098107814789.\n",
      "[I 2025-04-15 21:38:22,396] Trial 31 finished with value: 0.0704113021492958 and parameters: {'neurons': 5, 'epochs': 100}. Best is trial 20 with value: 0.06139098107814789.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'neurons': 8, 'epochs': 100}\n"
     ]
    }
   ],
   "source": [
    "mlp_optimizer = MLPOptimizer(n_steps, 32)\n",
    "study = mlp_optimizer.optimize(train_X, train_y, val_X, val_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first layer in the network must define the number of inputs to expect. Input must be three-dimensional, comprised of samples, timesteps, and features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/valeriagonzalez/Documents/aqi/env/lib/python3.9/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# design network\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, \n",
    "               input_shape=(train_X.shape[1], train_X.shape[2]),\n",
    "               activation=\"relu\"))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mse', optimizer=Adam(0.001))\n",
    "# fit network\n",
    "history = model.fit(\n",
    "    train_X, \n",
    "    train_y, \n",
    "    epochs=50, \n",
    "    batch_size=72, \n",
    "    validation_data=(val_X, val_y),\n",
    "    verbose=0, \n",
    "    shuffle=False,\n",
    "    )\n",
    "# plot history\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict values and evaluate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n"
     ]
    }
   ],
   "source": [
    "# make a prediction\n",
    "yhat = model.predict(test_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invert scaling for forecast\n",
    "test_X_reshaped = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n",
    "inv_yhat = np.concatenate((yhat, test_X_reshaped[:, 1:]), axis=1)\n",
    "inv_yhat = data_scaler.inverse_transform(inv_yhat)\n",
    "inv_yhat = inv_yhat[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invert scaling for actual\n",
    "test_y_reshaped = test_y.reshape((len(test_y), 1))\n",
    "inv_y = np.concatenate((test_y_reshaped, test_X_reshaped[:, 1:]), axis=1)\n",
    "inv_y = data_scaler.inverse_transform(inv_y)\n",
    "inv_y = inv_y[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 8.990\n"
     ]
    }
   ],
   "source": [
    "# calculate RMSE\n",
    "rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "print('Test RMSE: %.3f' % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: 32.041666666666664  Predict: 27.549011965468527\n",
      "Actual: 38.416666666666664  Predict: 29.706776524335144\n",
      "Actual: 30.374999999999996  Predict: 34.68147672154009\n",
      "Actual: 24.083333333333332  Predict: 28.75329812094569\n",
      "Actual: 18.29166666666667  Predict: 27.052470683678983\n",
      "Actual: 23.791666666666668  Predict: 24.255854977481068\n",
      "Actual: 15.11111111111111  Predict: 20.621170320734382\n",
      "Actual: 15.11111111111111  Predict: 17.12033283263445\n",
      "Actual: 75.9  Predict: 17.441500547900795\n",
      "Actual: 54.82916666666668  Predict: 46.38818802200257\n",
      "Actual: 58.00833333333333  Predict: 37.45154129229486\n",
      "Actual: 53.56666666666666  Predict: 38.74134216308594\n",
      "Actual: 42.29999999999999  Predict: 37.19076215960085\n",
      "Actual: 50.24583333333334  Predict: 31.50449686869979\n",
      "Actual: 88.2375  Predict: 35.737150254845616\n",
      "Actual: 58.4875  Predict: 52.33184154555201\n",
      "Actual: 49.5375  Predict: 38.40840817019343\n",
      "Actual: 53.17083333333333  Predict: 33.86831075437367\n",
      "Actual: 44.18333333333334  Predict: 33.41478636264801\n",
      "Actual: 47.020833333333336  Predict: 30.24105187766254\n",
      "Actual: 46.75  Predict: 31.535342416539788\n",
      "Actual: 42.84583333333333  Predict: 30.213720932230352\n",
      "Actual: 28.504166666666663  Predict: 26.724044550396503\n",
      "Actual: 19.583333333333336  Predict: 16.276854935660957\n",
      "Actual: 42.37083333333333  Predict: 11.337002724735067\n",
      "Actual: 53.933333333333344  Predict: 29.333045007288455\n",
      "Actual: 65.58749999999999  Predict: 35.977821689844134\n",
      "Actual: 74.98333333333333  Predict: 40.84097631387412\n",
      "Actual: 36.025  Predict: 44.52333852909505\n",
      "Actual: 42.75416666666666  Predict: 29.713298792019486\n",
      "Actual: 33.30833333333333  Predict: 34.99471778348088\n",
      "Actual: 40.3  Predict: 32.94675483703613\n",
      "Actual: 53.28333333333333  Predict: 34.219521164894104\n",
      "Actual: 68.625  Predict: 40.90043591596186\n",
      "Actual: 52.26521739130435  Predict: 45.78623049780726\n",
      "Actual: 62.96666666666667  Predict: 37.93021607212722\n",
      "Actual: 69.89999999999999  Predict: 43.37145608998834\n",
      "Actual: 82.48333333333332  Predict: 45.53026848062872\n",
      "Actual: 57.93333333333334  Predict: 50.84092246368527\n",
      "Actual: 51.408333333333324  Predict: 42.971012139320365\n",
      "Actual: 65.92916666666666  Predict: 40.63625618107617\n",
      "Actual: 77.80833333333334  Predict: 46.35477383099496\n",
      "Actual: 44.99166666666667  Predict: 49.36342630088329\n",
      "Actual: 51.51666666666667  Predict: 37.7426444824785\n",
      "Actual: 66.76666666666667  Predict: 40.771964948996896\n",
      "Actual: 53.25  Predict: 44.328034329041834\n",
      "Actual: 45.400000000000006  Predict: 40.296098940446974\n",
      "Actual: 41.24999999999999  Predict: 37.889503441751\n",
      "Actual: 34.670833333333334  Predict: 35.80576383173466\n",
      "Actual: 39.96666666666667  Predict: 29.91714330613613\n",
      "Actual: 50.14166666666667  Predict: 33.69779311195016\n",
      "Actual: 56.92916666666667  Predict: 41.09459526278078\n",
      "Actual: 63.8625  Predict: 42.15458867661654\n",
      "Actual: 50.90416666666667  Predict: 45.61413383111357\n",
      "Actual: 41.76666666666666  Predict: 40.305912661179896\n",
      "Actual: 31.520833333333332  Predict: 35.45600146651268\n",
      "Actual: 28.275  Predict: 29.678676683828233\n",
      "Actual: 36.94583333333333  Predict: 30.429001851007342\n",
      "Actual: 52.67083333333333  Predict: 34.86910652406514\n",
      "Actual: 44.5125  Predict: 39.91864179931581\n",
      "Actual: 56.75  Predict: 33.396558455750345\n",
      "Actual: 55.9875  Predict: 40.45003561526537\n",
      "Actual: 66.41666666666667  Predict: 42.47866461016237\n",
      "Actual: 57.2875  Predict: 46.79796786271035\n",
      "Actual: 41.19583333333333  Predict: 42.980517816916105\n",
      "Actual: 36.44166666666667  Predict: 34.479912211745976\n",
      "Actual: 37.6875  Predict: 30.569132371991873\n",
      "Actual: 33.36666666666667  Predict: 29.228196340426802\n",
      "Actual: 32.479166666666664  Predict: 25.925687696412204\n",
      "Actual: 35.141666666666666  Predict: 24.82787167895585\n",
      "Actual: 31.854166666666668  Predict: 25.046946136839686\n",
      "Actual: 34.333333333333336  Predict: 24.721590732783078\n",
      "Actual: 50.725  Predict: 26.811699801683424\n",
      "Actual: 36.225  Predict: 37.56933019645512\n",
      "Actual: 25.72083333333333  Predict: 28.615362710505725\n",
      "Actual: 20.22916666666667  Predict: 26.510628869198264\n",
      "Actual: 24.1625  Predict: 24.520155989192425\n",
      "Actual: 28.53333333333333  Predict: 27.35079227052629\n",
      "Actual: 28.733333333333334  Predict: 30.194011022523046\n",
      "Actual: 39.77916666666667  Predict: 27.662206903472544\n",
      "Actual: 33.38333333333333  Predict: 31.735873647779226\n",
      "Actual: 49.8375  Predict: 28.472806653007865\n",
      "Actual: 32.166666666666664  Predict: 38.031069880351424\n",
      "Actual: 19.337500000000002  Predict: 29.229787492379547\n",
      "Actual: 15.516666666666667  Predict: 22.35326534919441\n",
      "Actual: 34.84583333333333  Predict: 18.804079641774297\n",
      "Actual: 25.304166666666664  Predict: 29.156077863276003\n",
      "Actual: 23.791666666666668  Predict: 25.071047480404378\n",
      "Actual: 19.20416666666667  Predict: 22.833509451150896\n",
      "Actual: 17.708333333333332  Predict: 21.909518295153976\n",
      "Actual: 28.633333333333336  Predict: 20.561253505013884\n",
      "Actual: 25.858333333333334  Predict: 23.311912570893764\n",
      "Actual: 16.895833333333332  Predict: 18.354967300966383\n",
      "Actual: 20.7125  Predict: 14.901884988509119\n",
      "Actual: 22.958333333333336  Predict: 17.32993071936071\n",
      "Actual: 15.725  Predict: 21.979031746089458\n",
      "Actual: 16.537499999999998  Predict: 18.965901735797523\n",
      "Actual: 15.758333333333333  Predict: 18.722949083708226\n",
      "Actual: 20.091666666666665  Predict: 18.005144297331572\n",
      "Actual: 17.929166666666667  Predict: 16.58696402357891\n",
      "Actual: 20.508333333333333  Predict: 15.354465846531093\n",
      "Actual: 19.162500000000005  Predict: 19.204820720665158\n",
      "Actual: 24.4375  Predict: 18.342456383630633\n",
      "Actual: 29.791666666666668  Predict: 18.315882205590604\n",
      "Actual: 21.72083333333333  Predict: 23.274817628227176\n",
      "Actual: 13.25  Predict: 18.08411248959601\n",
      "Actual: 10.079166666666667  Predict: 13.41531307445839\n",
      "Actual: 13.220833333333331  Predict: 13.352881583757698\n",
      "Actual: 24.35  Predict: 14.463714242912829\n",
      "Actual: 26.3625  Predict: 19.282839315384628\n",
      "Actual: 26.8  Predict: 19.86827651038766\n",
      "Actual: 26.675  Predict: 22.72755959071219\n",
      "Actual: 26.120833333333337  Predict: 22.19085021149367\n",
      "Actual: 23.566666666666663  Predict: 21.28971731159836\n",
      "Actual: 28.383333333333336  Predict: 19.00410029757768\n",
      "Actual: 35.44166666666667  Predict: 23.40624168589711\n",
      "Actual: 33.1625  Predict: 27.26994913853705\n",
      "Actual: 35.37916666666667  Predict: 26.33917739521712\n",
      "Actual: 34.891666666666666  Predict: 28.48986302576959\n",
      "Actual: 19.59166666666667  Predict: 29.515884375199676\n",
      "Actual: 11.391666666666666  Predict: 22.70991866365075\n",
      "Actual: 15.245833333333332  Predict: 15.582019373681396\n",
      "Actual: 21.2125  Predict: 17.412916812859475\n",
      "Actual: 11.65416666666667  Predict: 19.9567540185526\n",
      "Actual: 12.47083333333333  Predict: 12.551852394919843\n",
      "Actual: 14.858333333333333  Predict: 10.93199724946171\n",
      "Actual: 20.320833333333333  Predict: 10.571872385870666\n",
      "Actual: 19.05833333333334  Predict: 15.720998371206225\n",
      "Actual: 25.4625  Predict: 15.844530945830048\n",
      "Actual: 20.183333333333337  Predict: 16.571706186141817\n",
      "Actual: 10.725  Predict: 15.070870904624462\n",
      "Actual: 6.979166666666667  Predict: 8.88623941889964\n",
      "Actual: 14.604166666666668  Predict: 8.126608680887147\n",
      "Actual: 23.545833333333334  Predict: 14.168362725526094\n",
      "Actual: 21.15416666666666  Predict: 20.588909472338855\n",
      "Actual: 35.737500000000004  Predict: 18.981152296625076\n",
      "Actual: 25.03333333333333  Predict: 25.700648844242096\n",
      "Actual: 24.879166666666663  Predict: 19.734622171893715\n",
      "Actual: 31.129166666666663  Predict: 19.95271550025791\n",
      "Actual: 28.333333333333332  Predict: 24.74268804937601\n",
      "Actual: 23.220833333333335  Predict: 25.17585006207228\n",
      "Actual: 23.97916666666667  Predict: 22.519574685767292\n",
      "Actual: 23.179166666666664  Predict: 20.541678212583065\n",
      "Actual: 21.35  Predict: 21.988577445037663\n",
      "Actual: 21.608333333333334  Predict: 20.803397240675984\n",
      "Actual: 22.96666666666667  Predict: 21.678335558995606\n",
      "Actual: 24.86923076923077  Predict: 21.6482261614874\n",
      "Actual: 24.86923076923077  Predict: 23.13759775105864\n",
      "Actual: 24.86923076923077  Predict: 22.154604206606745\n",
      "Actual: 24.86923076923077  Predict: 22.826226777769627\n",
      "Actual: 24.86923076923077  Predict: 24.0960545681417\n",
      "Actual: 24.86923076923077  Predict: 24.51857939045876\n",
      "Actual: 24.86923076923077  Predict: 25.36995119601488\n",
      "Actual: 24.86923076923077  Predict: 25.46530146189034\n",
      "Actual: 24.86923076923077  Predict: 23.4795777823776\n",
      "Actual: 24.86923076923077  Predict: 24.37999757453799\n",
      "Actual: 24.86923076923077  Predict: 23.118354757130145\n",
      "Actual: 24.86923076923077  Predict: 23.081119133345783\n",
      "Actual: 24.86923076923077  Predict: 23.716021507233382\n",
      "Actual: 24.86923076923077  Predict: 24.535866189189257\n",
      "Actual: 24.86923076923077  Predict: 24.173551672510804\n",
      "Actual: 24.86923076923077  Predict: 24.445354868657887\n",
      "Actual: 24.86923076923077  Predict: 25.28272769022733\n",
      "Actual: 24.86923076923077  Predict: 22.180396149121226\n",
      "Actual: 24.86923076923077  Predict: 19.92562225740403\n",
      "Actual: 24.86923076923077  Predict: 19.264575025439264\n",
      "Actual: 24.86923076923077  Predict: 21.82256523668766\n",
      "Actual: 24.86923076923077  Predict: 20.004306661896408\n",
      "Actual: 24.86923076923077  Predict: 21.495216041617095\n",
      "Actual: 24.86923076923077  Predict: 21.55789675619453\n",
      "Actual: 24.86923076923077  Predict: 21.79708255007863\n",
      "Actual: 24.86923076923077  Predict: 21.265521372109653\n",
      "Actual: 24.86923076923077  Predict: 21.781362647935747\n",
      "Actual: 24.86923076923077  Predict: 22.225736703164877\n",
      "Actual: 24.86923076923077  Predict: 20.636578541435302\n",
      "Actual: 24.86923076923077  Predict: 19.467787687294184\n",
      "Actual: 24.86923076923077  Predict: 20.22847959753126\n",
      "Actual: 24.86923076923077  Predict: 23.145371595583857\n",
      "Actual: 24.86923076923077  Predict: 23.782985719293357\n",
      "Actual: 24.86923076923077  Predict: 21.63549088202417\n",
      "Actual: 24.86923076923077  Predict: 18.63252144418657\n",
      "Actual: 24.86923076923077  Predict: 20.04148407280445\n",
      "Actual: 24.86923076923077  Predict: 18.79744337387383\n",
      "Actual: 24.86923076923077  Predict: 18.5943932229653\n",
      "Actual: 24.86923076923077  Predict: 19.10198646187782\n",
      "Actual: 24.86923076923077  Predict: 20.17191123496741\n",
      "Actual: 24.86923076923077  Predict: 22.04771323800087\n",
      "Actual: 24.86923076923077  Predict: 23.719920557178558\n",
      "Actual: 24.86923076923077  Predict: 23.632306540012358\n",
      "Actual: 24.86923076923077  Predict: 24.321539719030262\n",
      "Actual: 24.86923076923077  Predict: 19.5686075380072\n",
      "Actual: 24.86923076923077  Predict: 19.53912877999246\n",
      "Actual: 24.86923076923077  Predict: 21.393048805370928\n",
      "Actual: 24.86923076923077  Predict: 20.584543506614864\n",
      "Actual: 24.86923076923077  Predict: 21.8593957958743\n",
      "Actual: 24.86923076923077  Predict: 19.75393793191761\n",
      "Actual: 24.86923076923077  Predict: 23.940173825807868\n",
      "Actual: 24.86923076923077  Predict: 23.324459871277213\n",
      "Actual: 24.86923076923077  Predict: 25.15525240600109\n",
      "Actual: 24.86923076923077  Predict: 27.528060181066394\n",
      "Actual: 24.86923076923077  Predict: 27.46731989569962\n",
      "Actual: 24.86923076923077  Predict: 27.52950822636485\n",
      "Actual: 24.86923076923077  Predict: 29.74522612914443\n",
      "Actual: 24.86923076923077  Predict: 28.653889932483434\n",
      "Actual: 24.86923076923077  Predict: 27.921775693446396\n",
      "Actual: 24.86923076923077  Predict: 26.656295677274464\n",
      "Actual: 24.86923076923077  Predict: 26.268887772597374\n",
      "Actual: 24.86923076923077  Predict: 25.588928632438183\n",
      "Actual: 24.86923076923077  Predict: 26.529067797772587\n",
      "Actual: 24.86923076923077  Predict: 27.854236629232766\n",
      "Actual: 24.86923076923077  Predict: 25.68026948645711\n",
      "Actual: 24.86923076923077  Predict: 25.42962060701102\n",
      "Actual: 24.86923076923077  Predict: 22.213682999461888\n",
      "Actual: 24.86923076923077  Predict: 20.695738589763643\n",
      "Actual: 24.86923076923077  Predict: 24.202734515070915\n",
      "Actual: 24.86923076923077  Predict: 24.41173450704664\n",
      "Actual: 24.86923076923077  Predict: 26.665281077288093\n",
      "Actual: 24.86923076923077  Predict: 29.50876057446003\n",
      "Actual: 24.86923076923077  Predict: 30.520660450309514\n",
      "Actual: 24.86923076923077  Predict: 31.171870918944478\n",
      "Actual: 24.86923076923077  Predict: 31.319052474573255\n",
      "Actual: 24.86923076923077  Predict: 30.223653504252432\n",
      "Actual: 24.86923076923077  Predict: 28.107205534353852\n",
      "Actual: 24.86923076923077  Predict: 26.921555988676847\n",
      "Actual: 24.86923076923077  Predict: 27.915852533280848\n",
      "Actual: 24.86923076923077  Predict: 29.75607070289552\n",
      "Actual: 24.86923076923077  Predict: 29.29986124224961\n",
      "Actual: 24.86923076923077  Predict: 26.74610237944871\n",
      "Actual: 24.86923076923077  Predict: 27.844922569021584\n",
      "Actual: 24.86923076923077  Predict: 27.08845473062247\n",
      "Actual: 24.86923076923077  Predict: 29.550994016230106\n",
      "Actual: 24.86923076923077  Predict: 29.559796288236974\n",
      "Actual: 24.86923076923077  Predict: 30.24663303717971\n",
      "Actual: 24.86923076923077  Predict: 30.829974568635226\n",
      "Actual: 24.86923076923077  Predict: 30.621492428705096\n",
      "Actual: 24.86923076923077  Predict: 31.96274620555341\n",
      "Actual: 24.86923076923077  Predict: 31.418953046947717\n",
      "Actual: 24.86923076923077  Predict: 31.683659123256803\n",
      "Actual: 24.86923076923077  Predict: 30.12419680505991\n",
      "Actual: 24.86923076923077  Predict: 29.38919617757201\n",
      "Actual: 24.86923076923077  Predict: 29.302878609672188\n",
      "Actual: 24.86923076923077  Predict: 30.59424152597785\n",
      "Actual: 24.86923076923077  Predict: 30.922986617311835\n",
      "Actual: 24.86923076923077  Predict: 29.227352253720163\n",
      "Actual: 24.86923076923077  Predict: 28.829241669178007\n",
      "Actual: 24.86923076923077  Predict: 28.557200770452617\n",
      "Actual: 24.86923076923077  Predict: 29.660407542809843\n",
      "Actual: 24.86923076923077  Predict: 29.391449500992895\n",
      "Actual: 24.86923076923077  Predict: 30.504431185498834\n",
      "Actual: 24.86923076923077  Predict: 29.63092757202685\n",
      "Actual: 24.86923076923077  Predict: 28.17162778414786\n",
      "Actual: 24.86923076923077  Predict: 29.99965520724654\n",
      "Actual: 24.86923076923077  Predict: 30.42746406085789\n",
      "Actual: 24.86923076923077  Predict: 29.458394308760763\n",
      "Actual: 24.86923076923077  Predict: 29.62935582436621\n",
      "Actual: 24.86923076923077  Predict: 31.851605696976183\n",
      "Actual: 24.86923076923077  Predict: 31.77170609869063\n",
      "Actual: 24.86923076923077  Predict: 31.275227880850434\n",
      "Actual: 24.86923076923077  Predict: 33.91734540052712\n",
      "Actual: 24.86923076923077  Predict: 31.192082914710046\n",
      "Actual: 24.86923076923077  Predict: 30.72561343461275\n",
      "Actual: 24.86923076923077  Predict: 26.6248643623665\n",
      "Actual: 24.86923076923077  Predict: 28.665093485638497\n",
      "Actual: 24.86923076923077  Predict: 29.858974089473485\n",
      "Actual: 24.86923076923077  Predict: 29.11995919905603\n",
      "Actual: 24.86923076923077  Predict: 30.306631108373402\n",
      "Actual: 24.86923076923077  Predict: 30.003637938201425\n",
      "Actual: 24.86923076923077  Predict: 30.96782751083374\n",
      "Actual: 24.86923076923077  Predict: 31.818771209195255\n",
      "Actual: 24.86923076923077  Predict: 30.582062907144426\n",
      "Actual: 24.86923076923077  Predict: 30.436387609690428\n",
      "Actual: 24.86923076923077  Predict: 30.695847250521183\n",
      "Actual: 24.86923076923077  Predict: 30.049815302342175\n",
      "Actual: 24.86923076923077  Predict: 32.12259876392782\n",
      "Actual: 24.86923076923077  Predict: 31.12076243907213\n",
      "Actual: 24.86923076923077  Predict: 31.934110321477053\n",
      "Actual: 24.86923076923077  Predict: 32.20108184888959\n",
      "Actual: 24.86923076923077  Predict: 30.053810160979626\n",
      "Actual: 24.86923076923077  Predict: 29.549851588532327\n",
      "Actual: 24.86923076923077  Predict: 29.040537431463598\n",
      "Actual: 24.86923076923077  Predict: 29.716401053220032\n",
      "Actual: 24.86923076923077  Predict: 30.562401508167387\n",
      "Actual: 24.86923076923077  Predict: 31.7414014454931\n",
      "Actual: 24.86923076923077  Predict: 31.91725041717291\n",
      "Actual: 24.86923076923077  Predict: 30.261001915484666\n",
      "Actual: 24.86923076923077  Predict: 29.776942444592713\n",
      "Actual: 24.86923076923077  Predict: 31.849587650597094\n",
      "Actual: 24.86923076923077  Predict: 31.867788876593114\n",
      "Actual: 24.86923076923077  Predict: 28.61743169315159\n",
      "Actual: 24.86923076923077  Predict: 28.373784124851227\n",
      "Actual: 24.86923076923077  Predict: 28.22787840142846\n",
      "Actual: 24.86923076923077  Predict: 28.871450855582953\n",
      "Actual: 24.86923076923077  Predict: 29.96885817013681\n",
      "Actual: 24.86923076923077  Predict: 28.42342030405998\n",
      "Actual: 24.86923076923077  Predict: 29.137600126117466\n",
      "Actual: 24.86923076923077  Predict: 28.802019872888923\n",
      "Actual: 24.86923076923077  Predict: 29.582744289189577\n",
      "Actual: 24.86923076923077  Predict: 29.834095361456274\n",
      "Actual: 24.86923076923077  Predict: 28.637386582046748\n",
      "Actual: 24.86923076923077  Predict: 28.591308664903043\n",
      "Actual: 24.86923076923077  Predict: 27.691444320604205\n",
      "Actual: 24.86923076923077  Predict: 29.433534985035656\n",
      "Actual: 24.86923076923077  Predict: 25.018290848471224\n",
      "Actual: 24.86923076923077  Predict: 22.9288500148803\n",
      "Actual: 24.86923076923077  Predict: 26.830127815343438\n",
      "Actual: 24.86923076923077  Predict: 27.372811290994285\n",
      "Actual: 24.86923076923077  Predict: 26.170298202708363\n",
      "Actual: 24.86923076923077  Predict: 27.46934521868825\n",
      "Actual: 24.86923076923077  Predict: 27.750316942855715\n",
      "Actual: 24.86923076923077  Predict: 27.274977275729178\n",
      "Actual: 24.86923076923077  Predict: 27.827871047332884\n",
      "Actual: 24.86923076923077  Predict: 28.514935796707867\n",
      "Actual: 24.86923076923077  Predict: 28.501816069707274\n",
      "Actual: 24.86923076923077  Predict: 28.111341074109077\n",
      "Actual: 24.86923076923077  Predict: 28.49476018399\n",
      "Actual: 24.86923076923077  Predict: 27.214477118477227\n",
      "Actual: 24.86923076923077  Predict: 27.217458102852106\n",
      "Actual: 24.86923076923077  Predict: 27.881031531095505\n",
      "Actual: 24.86923076923077  Predict: 28.30101075284183\n",
      "Actual: 24.86923076923077  Predict: 26.547766258753835\n",
      "Actual: 24.86923076923077  Predict: 27.067427754588426\n",
      "Actual: 24.86923076923077  Predict: 28.549783479794858\n",
      "Actual: 24.86923076923077  Predict: 29.40668429583311\n",
      "Actual: 24.86923076923077  Predict: 25.293760243058205\n",
      "Actual: 24.86923076923077  Predict: 24.886609683930875\n",
      "Actual: 24.86923076923077  Predict: 26.622083484753965\n",
      "Actual: 24.86923076923077  Predict: 26.69806099049747\n",
      "Actual: 24.86923076923077  Predict: 29.470449225232006\n",
      "Actual: 24.86923076923077  Predict: 27.03614439740777\n",
      "Actual: 24.86923076923077  Predict: 28.51632077805698\n",
      "Actual: 24.86923076923077  Predict: 28.845643147081137\n",
      "Actual: 24.86923076923077  Predict: 26.906202342547477\n",
      "Actual: 24.86923076923077  Predict: 27.222192750126123\n",
      "Actual: 24.86923076923077  Predict: 27.7184308398515\n",
      "Actual: 24.86923076923077  Predict: 29.4117779225111\n",
      "Actual: 24.86923076923077  Predict: 26.54495506193489\n",
      "Actual: 24.86923076923077  Predict: 27.135815756581724\n",
      "Actual: 24.86923076923077  Predict: 27.79180331937969\n",
      "Actual: 24.86923076923077  Predict: 26.457489002496004\n",
      "Actual: 24.86923076923077  Predict: 26.43891909494996\n",
      "Actual: 24.86923076923077  Predict: 26.98830554075539\n",
      "Actual: 24.86923076923077  Predict: 24.20431111380458\n",
      "Actual: 24.86923076923077  Predict: 21.948500305227935\n",
      "Actual: 24.86923076923077  Predict: 21.284999643079935\n",
      "Actual: 24.86923076923077  Predict: 26.708093009516595\n",
      "Actual: 24.86923076923077  Predict: 27.78233159929514\n",
      "Actual: 24.86923076923077  Predict: 26.715872917883097\n",
      "Actual: 24.86923076923077  Predict: 26.905150872468948\n",
      "Actual: 24.86923076923077  Predict: 23.429090239852666\n",
      "Actual: 24.86923076923077  Predict: 24.90112409442663\n",
      "Actual: 24.86923076923077  Predict: 27.157404244318606\n",
      "Actual: 24.86923076923077  Predict: 28.838514495268463\n",
      "Actual: 24.86923076923077  Predict: 26.16884894464165\n",
      "Actual: 24.86923076923077  Predict: 26.95549288280308\n",
      "Actual: 24.86923076923077  Predict: 29.335295905172824\n",
      "Actual: 24.86923076923077  Predict: 25.81070635076612\n",
      "Actual: 24.86923076923077  Predict: 24.87519147079438\n",
      "Actual: 24.86923076923077  Predict: 25.72801738549024\n",
      "Actual: 24.86923076923077  Predict: 25.9949937639758\n",
      "Actual: 24.86923076923077  Predict: 26.355470936745405\n",
      "Actual: 24.86923076923077  Predict: 25.327374540828167\n",
      "Actual: 24.86923076923077  Predict: 25.443964017182587\n",
      "Actual: 24.86923076923077  Predict: 24.517267175205053\n",
      "Actual: 24.86923076923077  Predict: 26.413155046105384\n",
      "Actual: 24.86923076923077  Predict: 26.22192696016282\n",
      "Actual: 24.86923076923077  Predict: 27.18001994676888\n",
      "Actual: 24.86923076923077  Predict: 27.36060841679573\n",
      "Actual: 24.86923076923077  Predict: 25.839989853091538\n",
      "Actual: 24.86923076923077  Predict: 26.657197976857425\n",
      "Actual: 24.86923076923077  Predict: 27.91532376632094\n",
      "Actual: 24.86923076923077  Predict: 24.83490816038102\n",
      "Actual: 24.86923076923077  Predict: 23.631650432385506\n",
      "Actual: 24.86923076923077  Predict: 22.526733656786384\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(inv_y)):\n",
    "    print(f\"Actual: {inv_y[i]}  Predict: {inv_yhat[i]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
