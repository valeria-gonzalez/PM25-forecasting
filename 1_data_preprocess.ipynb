{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "The data used during this project has been obtained from a historical database of hourly sensor readings in Tlaquepaque, and although it's very useful information, it has some inconsistencies. The main problem this database has is incomplete data and a lack of standard practices in creating the datasets (various NULL representatives, missing features, etc.).\n",
    "\n",
    "Before performing any forecasting operations on the data, it will be preprocessed. Some of the changes made will be particular to the handling of time series data. \n",
    "\n",
    "These changes will mostly be comprised of: \n",
    "\n",
    "- Standarizing column names.\n",
    "- Extract relevant features.\n",
    "- Identify null values.\n",
    "- Determining feature data types.\n",
    "- Reducing number of instances.\n",
    "- Reframing the dataset to have a time series format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standarize column names and extract relevant features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After conducting an analysis of data from years between 2008 and 2023, the yearly readings of 2017, 2020 and 2023 have been specifically chosen because of the integrity of their data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of relevant features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forecasting the readings for the pollutants in Tlaquepaque requires the following features:\n",
    "\n",
    "_Exogenous variables:_\n",
    "\n",
    "- Date (Fecha)\n",
    "- Hour (Hora)\n",
    "- Station (Estaci√≥n)\n",
    "- Temperature (TMP)\n",
    "- Relative Humidity (RH)\n",
    "- Barometric Pressure (PBA)\n",
    "- Wind Speed (WS)\n",
    "- Wind Direction (WD)\n",
    "- Precipitation (PP)\n",
    "\n",
    "_Pollutants:_\n",
    "\n",
    "- Fine Particulate Matter less than 2.5 micrometers (PM2.5)\n",
    "- O3 (Ozone)\n",
    "- NO (Nitrogen Oxide)\n",
    "- NO2 (Nitrogen Dioxide)\n",
    "- NOX (Nitrogen Oxides)\n",
    "- CO (Carbon Monoxide)\n",
    "- PM10 (Fine Particulate Matter less than 10 micrometers)\n",
    "\n",
    "This project aims to forecast the PM2.5 readings for Tlaquepaque, but in case this projects is expanded to forecast all other pollutants, the dataset will be preprocessed and interpolated for all pollutants.\n",
    "\n",
    "\n",
    "All other features can be ignored. These are:\n",
    "\n",
    "_TMPI (Internal Temperature), Solar Radiation (RS), and UVI (UV Index)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the csv file\n",
    "filename = \"semadet-aire-2023\"\n",
    "filepath = f\"datasets/{filename}.csv\"\n",
    "\n",
    "df = pd.read_csv(filepath, encoding='utf-8')\n",
    "\n",
    "# Normalize column names\n",
    "df.columns = df.columns.str.lower().str.strip()\n",
    "\n",
    "# Rename certain columns \n",
    "df.rename({\"pm2.5\": \"pm25\", \n",
    "           \"date_time\": \"date\",\n",
    "           \"precipitacion\": \"pp\", \n",
    "           \"rad solar\": \"rs\",\n",
    "           \"presion barometrica\": \"pba\"},\n",
    "          axis=\"columns\",\n",
    "          inplace=True)\n",
    "\n",
    "# irrelevant_features = [\"rs\", \"nox\", \"no\"] # Irrelevant features 2017\n",
    "irrelevant_features = [\"rs\", \"nox\", \"no\", \"tmpi\", \"uvi\"] # Irrelevant features 2023\n",
    "df.drop(irrelevant_features, axis=\"columns\", inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>estacion</th>\n",
       "      <th>date</th>\n",
       "      <th>hora</th>\n",
       "      <th>o3</th>\n",
       "      <th>no2</th>\n",
       "      <th>so2</th>\n",
       "      <th>co</th>\n",
       "      <th>pm10</th>\n",
       "      <th>pm25</th>\n",
       "      <th>tmp</th>\n",
       "      <th>rh</th>\n",
       "      <th>ws</th>\n",
       "      <th>wd</th>\n",
       "      <th>pp</th>\n",
       "      <th>pba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aguilas</td>\n",
       "      <td>1/1/2023</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002</td>\n",
       "      <td>SE</td>\n",
       "      <td>SE</td>\n",
       "      <td>SE</td>\n",
       "      <td>61.8</td>\n",
       "      <td>58.1</td>\n",
       "      <td>12.6</td>\n",
       "      <td>88.7</td>\n",
       "      <td>0.38</td>\n",
       "      <td>190.77</td>\n",
       "      <td>0.25</td>\n",
       "      <td>SE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aguilas</td>\n",
       "      <td>1/1/2023</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002</td>\n",
       "      <td>SE</td>\n",
       "      <td>SE</td>\n",
       "      <td>SE</td>\n",
       "      <td>83.8</td>\n",
       "      <td>76.5</td>\n",
       "      <td>12.1</td>\n",
       "      <td>89.8</td>\n",
       "      <td>1.27</td>\n",
       "      <td>215.13</td>\n",
       "      <td>0</td>\n",
       "      <td>SE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aguilas</td>\n",
       "      <td>1/1/2023</td>\n",
       "      <td>2</td>\n",
       "      <td>0.003</td>\n",
       "      <td>SE</td>\n",
       "      <td>SE</td>\n",
       "      <td>SE</td>\n",
       "      <td>98.2</td>\n",
       "      <td>95</td>\n",
       "      <td>11.8</td>\n",
       "      <td>89.5</td>\n",
       "      <td>2.44</td>\n",
       "      <td>240.82</td>\n",
       "      <td>0</td>\n",
       "      <td>SE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  estacion      date  hora     o3 no2 so2  co  pm10  pm25   tmp    rh    ws  \\\n",
       "0  Aguilas  1/1/2023     0  0.002  SE  SE  SE  61.8  58.1  12.6  88.7  0.38   \n",
       "1  Aguilas  1/1/2023     1  0.002  SE  SE  SE  83.8  76.5  12.1  89.8  1.27   \n",
       "2  Aguilas  1/1/2023     2  0.003  SE  SE  SE  98.2    95  11.8  89.5  2.44   \n",
       "\n",
       "       wd    pp pba  \n",
       "0  190.77  0.25  SE  \n",
       "1  215.13     0  SE  \n",
       "2  240.82     0  SE  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See first three rows of data\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify null values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a variety of values used to identify for null data. All the identifiers for the null values will be standarized to see which columns to keep based on amount of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_with_null(row):\n",
    "    # Null value identifiers\n",
    "    null_values = [\"IO\", \"SE\", \"ND\", \"IF\", \"VE\", \"IR\", \"VZ\", \"IC\", \"IR 1000\",\n",
    "                   \"IR valor 1000\", \" \", \"\", \"-\", \"SD\"]\n",
    "    \n",
    "    # Columns that don't have null values\n",
    "    exclude_columns = [\"estacion\", \"date\", \"hora\"]\n",
    "    \n",
    "    # Replace null values by feature (column)\n",
    "    for column in row.index:\n",
    "        if column not in exclude_columns and row[column] in null_values:\n",
    "            row[column] = np.nan\n",
    "            \n",
    "    return row\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace null values for each column in dataframe\n",
    "df = df.apply(replace_with_null, axis=\"columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the Non-Null count for each feature, NO2 and SO2 have no useful data so they'll be dropped entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 87600 entries, 0 to 87599\n",
      "Data columns (total 15 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   estacion  87600 non-null  object \n",
      " 1   date      87600 non-null  object \n",
      " 2   hora      87600 non-null  int64  \n",
      " 3   o3        49551 non-null  object \n",
      " 4   no2       0 non-null      float64\n",
      " 5   so2       0 non-null      float64\n",
      " 6   co        28843 non-null  object \n",
      " 7   pm10      57768 non-null  object \n",
      " 8   pm25      50068 non-null  object \n",
      " 9   tmp       44625 non-null  object \n",
      " 10  rh        46416 non-null  object \n",
      " 11  ws        49970 non-null  object \n",
      " 12  wd        39858 non-null  object \n",
      " 13  pp        38714 non-null  object \n",
      " 14  pba       39048 non-null  object \n",
      "dtypes: float64(2), int64(1), object(12)\n",
      "memory usage: 10.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unuseful columns\n",
    "df.drop([\"no2\", \"so2\"], axis=\"columns\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify data tyoes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the null values have been correctly identified, every feature that represents a pollutant or exogenous variable can be correctly cast as a float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns that should be treated as floats\n",
    "float_cols = [\"o3\", \"co\", \"pm10\", \"pm25\", \"tmp\", \"rh\", \"ws\", \"wd\", \"pp\"]\n",
    "\n",
    "# Cast columns to floats\n",
    "df[float_cols] = df[float_cols].astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract relevant instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The yearly dataset provides hourly readings for many regions in Guadalajara. As this project only aims to forecast PM2.5 for Tlaquepaque, all the instances that belong to this region will be extracted, making the feature Station (Estacion) no longer useful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract instances that belong to the target region\n",
    "df = df[df[\"estacion\"] == \"Tlaquepaque\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unuseful column\n",
    "df.drop(\"estacion\", axis=\"columns\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reframe dataset as timeseries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to treat the data as a timeseries, the \"Date\" column will be used as the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use date as index\n",
    "df.index = pd.to_datetime(df['date'], format='%m/%d/%Y')\n",
    "# Drop the date column\n",
    "df.drop(\"date\", axis=\"columns\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the dates are the index, the instances will be sorted in ascending order to preserve seasonal trends and patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort dates \n",
    "df = df.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce number of instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this project is to forecast the daily PM2.5 readings for Tlaquepaque. Only one entry per day will be preserved by calculating the average of all the readings that belong to the same day.\n",
    "\n",
    "In the special case of Wind Direction, the fact that the measurement is taken in degrees will be taken into account by calculating the cirular mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def circular_mean(angles):\n",
    "    angles_rad = np.deg2rad(angles)  \n",
    "    mean_sin = np.mean(np.sin(angles_rad))\n",
    "    mean_cos = np.mean(np.cos(angles_rad))\n",
    "    mean_angle = np.arctan2(mean_sin, mean_cos)  \n",
    "    return np.rad2deg(mean_angle) % 360 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify average as the aggregating function every feature except wind direction and hour\n",
    "aggregation_functions = {col: \"mean\" for col in df.columns if col not in [\"wd\", \"hora\"]}\n",
    "\n",
    "# Specify circular mean for Wind Direction\n",
    "aggregation_functions[\"wd\"] = circular_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group all instances by date and apply average functions\n",
    "df_daily = df.groupby(\"date\").agg(aggregation_functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reducing the number of instances and analyzing the Non-Null count by column, PBA and CO have no relevant information at all, so they'll be eliminated.\n",
    "\n",
    "It can also be seen that now there are maximum 365 instances per feature (one for every day of the year.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 365 entries, 2023-01-01 to 2023-12-31\n",
      "Data columns (total 10 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   o3      180 non-null    float64\n",
      " 1   co      0 non-null      float64\n",
      " 2   pm10    365 non-null    float64\n",
      " 3   pm25    365 non-null    float64\n",
      " 4   tmp     131 non-null    float64\n",
      " 5   rh      315 non-null    float64\n",
      " 6   ws      319 non-null    float64\n",
      " 7   pp      365 non-null    float64\n",
      " 8   pba     0 non-null      object \n",
      " 9   wd      257 non-null    float64\n",
      "dtypes: float64(9), object(1)\n",
      "memory usage: 31.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df_daily.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unuseful columns\n",
    "df_daily.drop([\"pba\", \"co\"], axis=\"columns\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save pre processed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data has been properly processed, it'll be saved as its own file. This process has been repeated for each yearly dataset that's been found useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_daily.to_csv(f\"datasets/preprocess/{filename}-processed.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
